{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xe9dsPN57fli"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "file_path = \"cleaned_infopankki-fa.csv\"\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "8yQYvmM_72pI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all text from the English and Persian columns\n",
        "english_corpus = df['English'].dropna().tolist()\n",
        "persian_corpus = df['Persian'].dropna().tolist()\n",
        "\n",
        "# Pre-tokenizer: Load GPT-2 tokenizer to mimic pre-tokenization step\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "C-rPS-Z_4uqQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute word frequencies and alphabet for BPE\n",
        "def compute_word_frequencies(corpus):\n",
        "    word_freqs = defaultdict(int)\n",
        "    for text in corpus:\n",
        "        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "        new_words = [word for word, offset in words_with_offsets]\n",
        "        for word in new_words:\n",
        "            word_freqs[word] += 1\n",
        "    return word_freqs\n",
        "\n",
        "\n",
        "# Function to compute base vocabulary (characters)\n",
        "def compute_alphabet(word_freqs):\n",
        "    alphabet = []\n",
        "    for word in word_freqs.keys():\n",
        "        for letter in word:\n",
        "            if letter not in alphabet:\n",
        "                alphabet.append(letter)\n",
        "    alphabet.sort()\n",
        "    return alphabet\n",
        "\n",
        "\n",
        "# Function to compute frequency of pairs\n",
        "def compute_pair_freqs(splits, word_freqs):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs\n",
        "\n",
        "\n",
        "# Function to merge pairs in the splits\n",
        "def merge_pair(a, b, splits, word_freqs):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2:]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits\n",
        "\n",
        "\n",
        "# Function to perform BPE tokenization\n",
        "def bpe_tokenization(corpus, vocab_size=50):\n",
        "    # Compute word frequencies\n",
        "    word_freqs = compute_word_frequencies(corpus)\n",
        "\n",
        "    # Compute base vocabulary (characters)\n",
        "    alphabet = compute_alphabet(word_freqs)\n",
        "\n",
        "    # Initialize the vocabulary with special tokens and alphabet\n",
        "    vocab = [\"\"] + alphabet.copy()\n",
        "\n",
        "    # Split each word into individual characters\n",
        "    splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "\n",
        "    # BPE training: merge pairs until reaching desired vocab size\n",
        "    merges = {}\n",
        "\n",
        "    while len(vocab) < vocab_size:\n",
        "        pair_freqs = compute_pair_freqs(splits, word_freqs)\n",
        "        best_pair = \"\"\n",
        "        max_freq = None\n",
        "        for pair, freq in pair_freqs.items():\n",
        "            if max_freq is None or max_freq < freq:\n",
        "                best_pair = pair\n",
        "                max_freq = freq\n",
        "        splits = merge_pair(*best_pair, splits, word_freqs)\n",
        "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "        vocab.append(best_pair[0] + best_pair[1])\n",
        "\n",
        "    # Tokenization function to apply learned merges\n",
        "    def tokenize(text):\n",
        "        pre_tokenize_result = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "        splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "        for pair, merge in merges.items():\n",
        "            for idx, split in enumerate(splits):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [merge] + split[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[idx] = split\n",
        "\n",
        "        return sum(splits, [])\n",
        "\n",
        "    # Apply the tokenization to the entire corpus\n",
        "    tokenized_corpus = [tokenize(text) for text in corpus]\n",
        "    return tokenized_corpus"
      ],
      "metadata": {
        "id": "zz3-0TTd79qh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the English and Persian text separately\n",
        "english_tokenized = bpe_tokenization(english_corpus)\n",
        "persian_tokenized = bpe_tokenization(persian_corpus)\n",
        "\n",
        "# Create new DataFrames for tokenized results\n",
        "english_df = pd.DataFrame({'English_Tokenized': [' '.join(tokens) for tokens in english_tokenized]})\n",
        "persian_df = pd.DataFrame({'Persian_Tokenized': [' '.join(tokens) for tokens in persian_tokenized]})\n",
        "\n",
        "# Save the tokenized results to separate CSV files\n",
        "english_output_file_path_csv = \"tokenized_english_bpe.csv\"\n",
        "persian_output_file_path_csv = \"tokenized_persian_bpe.csv\"\n",
        "\n",
        "english_df.to_csv(english_output_file_path_csv, index=False)\n",
        "persian_df.to_csv(persian_output_file_path_csv, index=False)\n",
        "\n",
        "# # Save the tokenized results to separate EXCEL files\n",
        "# english_output_file_path_xlsx = \"tokenized_english_bpe.xlsx\"\n",
        "# persian_output_file_xlsx = \"tokenized_persian_bpe.xlsx\"\n",
        "\n",
        "# english_df.to_excel(english_output_file_path_xlsx, index=False)\n",
        "# persian_df.to_excel(persian_output_file_xlsx, index=False)"
      ],
      "metadata": {
        "id": "iLdcA1Kl8BDF"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}