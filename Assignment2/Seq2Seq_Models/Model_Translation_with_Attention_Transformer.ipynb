{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMoE4okKpuo5",
        "outputId": "cc87f8fd-d2de-4504-bc16-324395527507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=68103cd46aa4b39912a4e994a63f02b019d0978d0fd46c7fd9c4e5d41059675b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install fasttext\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "!pip install rouge-score\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo2baFDyUbAw",
        "outputId": "24ce9e34-5f45-45f9-aca7-619830137705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-11 09:02:18--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.176.126, 18.238.176.44, 18.238.176.19, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.176.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  63.4MB/s    in 17s     \n",
            "\n",
            "2024-09-11 09:02:35 (75.4 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "--2024-09-11 09:03:35--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.176.19, 18.238.176.44, 18.238.176.115, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.176.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258183862 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fa.300.vec.gz’\n",
            "\n",
            "cc.fa.300.vec.gz    100%[===================>]   1.17G  36.8MB/s    in 16s     \n",
            "\n",
            "2024-09-11 09:03:51 (76.9 MB/s) - ‘cc.fa.300.vec.gz’ saved [1258183862/1258183862]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download English FastText vectors\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gunzip cc.en.300.vec.gz\n",
        "\n",
        "# Download Persian FastText vectors\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
        "!gunzip cc.fa.300.vec.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITuSr669UAur",
        "outputId": "d4bf305e-313b-4de6-fc62-c86bd1ef1ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agZy5EQSbfnS",
        "outputId": "1b10f5b8-46e9-4ca3-a873-76d8e668ba87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Usage: 87.5%\n",
            "                                             English  \\\n",
            "0                         the passion of joan of arc   \n",
            "1                           a film by carl th dreyer   \n",
            "2  at the bibliotheque de la chambre des deputes ...   \n",
            "3  the record of the trial of joan of arc the tri...   \n",
            "4  the questions of the judges and joans response...   \n",
            "\n",
            "                                             Persian         Source  \n",
            "0                                      مصایب ژاندارک  OpenSubtitles  \n",
            "1                         فیلمی از کارل تیودور درایر  OpenSubtitles  \n",
            "2  در کتابخانه مجلس نمایندگان پاریس یکی از خارق ا...  OpenSubtitles  \n",
            "3  پرونده محاکمه ژاندارک محاکمه ای که منجر به مرگ...  OpenSubtitles  \n",
            "4        پرسشهای قضات و پاسخهای ژان عینا پرونده شدند  OpenSubtitles  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, Dropout, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import psutil\n",
        "\n",
        "# Check memory usage\n",
        "print(f\"Memory Usage: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Function to load FastText vectors from .vec file\n",
        "def load_fasttext_vectors(filepath):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            values = line.rstrip().split(' ')\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "# Load FastText vectors\n",
        "english_fasttext = load_fasttext_vectors('cc.en.300.vec')\n",
        "persian_fasttext = load_fasttext_vectors('cc.fa.300.vec')\n",
        "\n",
        "# Load the preprocessed data\n",
        "data = pd.read_csv('df_open_10000.csv')\n",
        "print(data.head())\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_VOCAB_SIZE = 5000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "embedding_dim = 300\n",
        "d_model = 300\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "dropout_rate = 0.2\n",
        "batch_size = 32\n",
        "accumulation_steps = 4\n",
        "epochs=15\n",
        "\n",
        "# Tokenizer for English\n",
        "english_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "english_tokenizer.fit_on_texts(data['English'])\n",
        "english_sequences = english_tokenizer.texts_to_sequences(data['English'])\n",
        "\n",
        "# Tokenizer for Persian\n",
        "persian_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "persian_tokenizer.fit_on_texts(data['Persian'])\n",
        "persian_sequences = persian_tokenizer.texts_to_sequences(data['Persian'])\n",
        "\n",
        "# Padding sequences\n",
        "english_padded = pad_sequences(english_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "persian_padded = pad_sequences(persian_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(\n",
        "    english_padded, persian_padded, data.index, test_size=0.2, random_state=42)\n",
        "\n",
        "X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
        "    X_temp, y_temp, idx_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create the embedding matrix for English\n",
        "def create_embedding_matrix(tokenizer, embedding_dim, embeddings_index):\n",
        "    embedding_matrix = np.zeros((MAX_VOCAB_SIZE, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i < MAX_VOCAB_SIZE:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            else:\n",
        "                embedding_matrix[i] = np.zeros(embedding_dim)\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create embedding matrices for both English and Persian\n",
        "english_embedding_matrix = create_embedding_matrix(english_tokenizer, embedding_dim, english_fasttext)\n",
        "persian_embedding_matrix = create_embedding_matrix(persian_tokenizer, embedding_dim, persian_fasttext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5MfbqvhzYJr"
      },
      "outputs": [],
      "source": [
        "# Transformer model with improvements\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    angles = np.arange(seq_len)[:, np.newaxis] / np.power(10000, 2 * (np.arange(d_model)[np.newaxis, :] // 2) / d_model)\n",
        "    pos_encoding = np.zeros((seq_len, d_model))\n",
        "    pos_encoding[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, num_layers, d_model, num_heads, dropout, embedding_matrix, name=\"transformer\"):\n",
        "        super(TransformerModel, self).__init__(name=name)\n",
        "        self.embedding = Embedding(vocab_size, d_model, weights=[embedding_matrix], trainable=True)\n",
        "        self.pos_encoding = get_positional_encoding(MAX_SEQUENCE_LENGTH, d_model)\n",
        "        self.enc_layers = [\n",
        "            {\n",
        "                \"attention\": tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model),\n",
        "                \"norm\": LayerNormalization(epsilon=1e-6),\n",
        "                \"ffn\": Dense(d_model, activation='relu')\n",
        "            }\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x += self.pos_encoding[:tf.shape(x)[1], :]\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.enc_layers:\n",
        "            attn_output = layer[\"attention\"](x, x)\n",
        "            attn_output = layer[\"norm\"](attn_output + x)\n",
        "            x = layer[\"ffn\"](attn_output)\n",
        "        return x\n",
        "\n",
        "def build_transformer(vocab_size, d_model, num_layers, num_heads, dropout_rate, embedding_matrix):\n",
        "    inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    transformer_model = TransformerModel(vocab_size, num_layers, d_model, num_heads, dropout_rate, embedding_matrix)(inputs)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(transformer_model)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build the transformer model for English\n",
        "vocab_size = MAX_VOCAB_SIZE\n",
        "model = build_transformer(vocab_size, d_model, num_layers, num_heads, dropout_rate, english_embedding_matrix)\n",
        "\n",
        "# Learning rate scheduler with warm-up\n",
        "def lr_scheduler(epoch, lr):\n",
        "    warmup_epochs = 5\n",
        "    if epoch < warmup_epochs:\n",
        "        return float(lr * (epoch + 1) / warmup_epochs)  # Ensure the learning rate is a float\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))  # Ensure the learning rate is a float\n",
        "\n",
        "\n",
        "# Compile the model without label smoothing\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Prepare target data (Shift Persian sequences by one step)\n",
        "decoder_target_data = np.zeros_like(y_train)\n",
        "decoder_target_data[:, :-1] = y_train[:, 1:]\n",
        "decoder_target_data[:, -1] = 0\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_filepath = 'epoch-{epoch:02d}-val_loss-{val_loss:.2f}.keras'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode=\"min\")\n",
        "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brsYriTxi4s5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81725ee-227e-402c-c9f0-0b0e115abf65"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564ms/step - accuracy: 0.8648 - loss: 1.6328\n",
            "Epoch 1: val_loss improved from inf to 1.03581, saving model to epoch-01-val_loss-1.04.keras\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 592ms/step - accuracy: 0.8649 - loss: 1.6322 - val_accuracy: 0.8657 - val_loss: 1.0358 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552ms/step - accuracy: 0.8873 - loss: 0.8646\n",
            "Epoch 2: val_loss improved from 1.03581 to 1.03540, saving model to epoch-02-val_loss-1.04.keras\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 598ms/step - accuracy: 0.8873 - loss: 0.8646 - val_accuracy: 0.8662 - val_loss: 1.0354 - learning_rate: 4.0000e-05\n",
            "Epoch 3/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557ms/step - accuracy: 0.8850 - loss: 0.8716\n",
            "Epoch 3: val_loss improved from 1.03540 to 1.02924, saving model to epoch-03-val_loss-1.03.keras\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m607s\u001b[0m 584ms/step - accuracy: 0.8850 - loss: 0.8716 - val_accuracy: 0.8663 - val_loss: 1.0292 - learning_rate: 2.4000e-05\n",
            "Epoch 4/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 563ms/step - accuracy: 0.8857 - loss: 0.8625\n",
            "Epoch 4: val_loss did not improve from 1.02924\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m642s\u001b[0m 604ms/step - accuracy: 0.8857 - loss: 0.8625 - val_accuracy: 0.8664 - val_loss: 1.0339 - learning_rate: 1.9200e-05\n",
            "Epoch 5/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566ms/step - accuracy: 0.8863 - loss: 0.8537\n",
            "Epoch 5: val_loss improved from 1.02924 to 1.02414, saving model to epoch-05-val_loss-1.02.keras\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 595ms/step - accuracy: 0.8863 - loss: 0.8537 - val_accuracy: 0.8667 - val_loss: 1.0241 - learning_rate: 1.9200e-05\n",
            "Epoch 6/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561ms/step - accuracy: 0.8855 - loss: 0.8568\n",
            "Epoch 6: val_loss did not improve from 1.02414\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 602ms/step - accuracy: 0.8855 - loss: 0.8568 - val_accuracy: 0.8663 - val_loss: 1.0376 - learning_rate: 1.7373e-05\n",
            "Epoch 7/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570ms/step - accuracy: 0.8851 - loss: 0.8570\n",
            "Epoch 7: val_loss did not improve from 1.02414\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 591ms/step - accuracy: 0.8851 - loss: 0.8570 - val_accuracy: 0.8662 - val_loss: 1.0366 - learning_rate: 1.5720e-05\n",
            "Epoch 8/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575ms/step - accuracy: 0.8857 - loss: 0.8473\n",
            "Epoch 8: val_loss did not improve from 1.02414\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 616ms/step - accuracy: 0.8857 - loss: 0.8473 - val_accuracy: 0.8665 - val_loss: 1.0388 - learning_rate: 1.4224e-05\n",
            "Epoch 9/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567ms/step - accuracy: 0.8866 - loss: 0.8399\n",
            "Epoch 9: val_loss did not improve from 1.02414\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 590ms/step - accuracy: 0.8866 - loss: 0.8399 - val_accuracy: 0.8664 - val_loss: 1.0420 - learning_rate: 1.2870e-05\n",
            "Epoch 10/15\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578ms/step - accuracy: 0.8865 - loss: 0.8374\n",
            "Epoch 10: val_loss did not improve from 1.02414\n",
            "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 619ms/step - accuracy: 0.8865 - loss: 0.8374 - val_accuracy: 0.8664 - val_loss: 1.0419 - learning_rate: 1.1645e-05\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 705ms/step - accuracy: 0.8669 - loss: 1.0380\n",
            "Test Accuracy: 87.12%\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    np.expand_dims(decoder_target_data, -1),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size // accumulation_steps,\n",
        "    validation_data=(X_val, np.expand_dims(y_val, -1)),\n",
        "    callbacks=[early_stopping, lr_schedule, checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, np.expand_dims(y_test, -1))\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECmcp4kcqLDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ea018c-ec02-4ba4-d8f7-063f6f29c80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.02113649314394344\n",
            "ROUGE-1: 0.0, ROUGE-L: 0.0\n",
            "Predictions saved to 'translation_predictions_transformer.csv'\n"
          ]
        }
      ],
      "source": [
        "beam_width = 3\n",
        "\n",
        "# Beam search decoding\n",
        "def beam_search_decoding(model, input_seq, beam_width=3):\n",
        "    sequences = [[list(), 0.0]]  # Start with an empty sequence and a probability of 0\n",
        "    for row in input_seq:\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            # Predict next words\n",
        "            yhat = model.predict(np.expand_dims(np.expand_dims(row, axis=0), axis=-1), verbose=0)\n",
        "            yhat = yhat[0, -1, :]  # Extract the last time step prediction probabilities\n",
        "\n",
        "            # Find top beam_width candidates\n",
        "            top_indices = np.argsort(yhat)[-beam_width:]  # Select top k candidates\n",
        "\n",
        "            for j in top_indices:\n",
        "                candidate = [seq + [j], score + np.log(yhat[j])]  # Append predicted word and its log probability\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        # Order all candidates by their scores and select the best beam_width sequences\n",
        "        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
        "        sequences = ordered[:beam_width]\n",
        "\n",
        "    return sequences[0][0]  # Return the best sequence\n",
        "\n",
        "# Limit the number of test samples to avoid memory issues\n",
        "num_test_samples = 50  # Adjust this number based on available memory\n",
        "X_test_subset = X_test[:num_test_samples]\n",
        "y_test_subset = y_test[:num_test_samples]\n",
        "\n",
        "# Predictions and decoding\n",
        "y_pred = [beam_search_decoding(model, seq, beam_width=beam_width) for seq in X_test_subset]\n",
        "\n",
        "# Decoding sequences\n",
        "def decode_sequence(sequence, tokenizer):\n",
        "    reverse_word_map = {index: word for word, index in tokenizer.word_index.items()}\n",
        "    decoded_sentence = ' '.join([reverse_word_map.get(idx, '') for idx in sequence if idx != 0])\n",
        "    return decoded_sentence\n",
        "\n",
        "y_pred_decoded = [decode_sequence(seq, persian_tokenizer) for seq in y_pred]\n",
        "y_true_decoded = [decode_sequence(seq, persian_tokenizer) for seq in y_test_subset]\n",
        "\n",
        "# Evaluate BLEU score\n",
        "def evaluate_bleu_score(y_true, y_pred):\n",
        "    references = [[sentence] for sentence in y_true]\n",
        "    hypotheses = [sentence for sentence in y_pred]\n",
        "    return corpus_bleu(references, hypotheses)\n",
        "\n",
        "bleu_score = evaluate_bleu_score(y_true_decoded, y_pred_decoded)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "\n",
        "# Evaluate ROUGE score\n",
        "def evaluate_rouge_score(y_true, y_pred):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [scorer.score(true, pred) for true, pred in zip(y_true, y_pred)]\n",
        "    avg_rouge = {\n",
        "        \"rouge1\": np.mean([score['rouge1'].fmeasure for score in rouge_scores]),\n",
        "        \"rougeL\": np.mean([score['rougeL'].fmeasure for score in rouge_scores]),\n",
        "    }\n",
        "    return avg_rouge\n",
        "\n",
        "rouge_score = evaluate_rouge_score(y_true_decoded, y_pred_decoded)\n",
        "print(f\"ROUGE-1: {rouge_score['rouge1']}, ROUGE-L: {rouge_score['rougeL']}\")\n",
        "\n",
        "# Save predictions\n",
        "output_data = []\n",
        "for i in range(len(X_test_subset)):\n",
        "    original_english = data['English'].iloc[idx_test[i]]\n",
        "    predicted_persian = decode_sequence(y_pred[i], persian_tokenizer)\n",
        "    actual_persian = decode_sequence(y_test_subset[i], persian_tokenizer)\n",
        "\n",
        "    output_data.append({\n",
        "        'Original English': original_english,\n",
        "        'Predicted Persian': predicted_persian,\n",
        "        'Actual Persian': actual_persian\n",
        "    })\n",
        "\n",
        "    # Convert the predictions into a DataFrame and save to a CSV file\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv('translation_predictions_transformer.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'translation_predictions_transformer.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EcdaqrqHbJ3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}